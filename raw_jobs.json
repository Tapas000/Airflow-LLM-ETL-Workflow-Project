[
  {
    "title": "Global People Analytics Analyst - Data Engineering",
    "company": "Boston Consulting Group",
    "location": "Gurugram, Haryana, India",
    "description": "Who We Are\n\nBoston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation-inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.\n\nTo succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital ventures\u2014and business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.\n\nWhat You'll Do\n\nAs a Data Engineer, you will play a crucial role in designing, building, and maintaining the data\n\ninfrastructure and systems required for efficient and reliable data processing. You will\n\ncollaborate with cross-functional teams, including data scientists, analysts, to ensure the\n\navailability, integrity, and accessibility of data for various business needs. This role requires a\n\nstrong understanding of data management principles, database technologies, data integration,\n\nand data warehousing concepts.\n\nKey Responsibilities\n\n\u00b7 Develop and maintain data warehouse solutions, including data modeling, schema\n\ndesign, and indexing strategies\n\n\u00b7 Optimize data processing workflows for improved performance, reliability, and\n\nscalability\n\n\u00b7 Identify and integrate diverse data sources, both internal and external, into a\n\ncentralized data platform\n\n\u00b7 Implement and manage data lakes, data marts, or other storage solutions as required\n\n\u00b7 Ensure data privacy and compliance with relevant data protection regulations\n\n\u00b7 Define and implement data governance policies, standards, and best practices\n\n\u00b7 Transform raw data into usable formats for analytics, reporting, and machine\n\nlearning purposes\n\n\u00b7 Perform data cleansing, normalization, aggregation, and enrichment operations to\n\nenhance data quality and usability\n\n\u00b7 Collaborate with data analysts and data scientists to understand data requirements\n\nand implement appropriate data transformations\n\nWhat You'll Bring\n\n\u00b7 Bachelor's or Master's degree in Computer Science, Data Science, Information\n\nSystems, or a related field\n\n\u00b7 Proficiency in SQL and experience with relational databases (e.g., Snowflake, MySQL,\n\nPostgreSQL, Oracle)\n\n\u00b7 3+ years of experience in data engineering or a similar role\n\n\u00b7 Hands-on programming skills in languages such as Python or Java is a plus\n\n\u00b7 Familiarity with cloud-based data platforms (e.g., AWS, Azure, GCP) and related\n\nservices (e.g., S3, Redshift, BigQuery) is good to have\n\n\u00b7 Knowledge of data modeling and database design principles\n\n\u00b7 Familiarity with data visualization tools (e.g., Tableau, Power BI) is a plus\n\n\u00b7 Strong problem-solving and analytical skills with attention to detail\n\n\u00b7 Experience with HR data analysis and HR domain knowledge is preferred\n\nWho You'll Work With\n\nAs part of the People Analytics team, you will modernize HR platforms, capabilities &\n\nengagement, automate/digitize core HR processes and operations and enable greater\n\nefficiency. You will collaborate with the global people team and colleagues across BCG to\n\nmanage the life cycle of all BCG employees.\n\nThe People Management Team (PMT) is comprised of several centers of expertise including\n\nHR Operations, People Analytics, Career Development, Learning & Development, Talent\n\nAcquisition & Branding, Compensation, and Mobility. Our centers of expertise work together\n\nto build out new teams and capabilities by sourcing, acquiring and retaining the best, diverse\n\ntalent for BCG\u2019s Global Services Business.\n\nWe develop talent and capabilities, while enhancing managers\u2019 effectiveness, and building\n\naffiliation and engagement in our new global offices. The PMT also harmonizes process\n\nefficiencies, automation, and global standardization. Through analytics and digitalization, we\n\nare always looking to expand our PMT capabilities and coverage\n\nBoston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, age, religion, sex, sexual orientation, gender identity / expression, national origin, disability, protected veteran status, or any other characteristic protected under national, provincial, or local law, where applicable, and those with criminal histories will be considered in a manner consistent with applicable state and local laws.\n\nBCG is an E - Verify Employer. Click here for more information on E-Verify."
  },
  {
    "title": "Data Engineer Lead Job in Hyderabad, India",
    "company": "Virtusa",
    "location": "Hyderabad, Telangana, India",
    "description": "We are seeking a Data Engineer with strong expertise in SQL and ETL processes to support banking data quality data pipelines, regulatory reporting, and data quality initiatives. The role involves building and optimizing data structures, implementing validation rules, and collaborating with governance and compliance teams. Experience in the banking domain and tools like Informatica and Azure Data Factory is essential. Strong proficiency in SQL for writing complex queries, joins, data transformations, and aggregationsProven experience in building tables, views, and data structures within enterprise Data Warehouses and Data LakesStrong understanding of data warehousing concepts, such as Slowly Changing Dimensions (SCDs), data normalization, and star/snowflake schemasPractical experience in Azure Data Factory (ADF) for orchestrating data pipelines and managing ingestion workflows Exposure to data cataloging, metadata management, and lineage tracking using Informatica EDC or AxonExperience implementing Data Quality rules for banking use cases such as completeness, consistency, uniqueness, and validityFamiliarity with banking systems and data domains such as Flexcube, HRMS, CRM, Risk, Compliance, and IBG reportingUnderstanding of regulatory and audit readiness needs for Central Bank and internal governance forums Write optimized SQL scripts to extract, transform, and load (ETL) data from multiple banking source systemsDesign and implement staging and reporting layer structures, aligned to business requirements and regulatory frameworksApply data validation logic based on predefined business rules and data governance requirementsCollaborate with Data Governance, Risk, and Compliance teams to embed lineage, ownership, and metadata into datasetsMonitor scheduled jobs and resolve ETL failures to ensure SLA adherence for reporting and operational dashboardsSupport production deployment, UAT sign off, and issue resolution for data products across business units 3 to 6 years in banking-focused data engineering roles with hands on SQL, ETL, and DQ rule implementationBachelors or Master's Degree in Computer Science, Information Systems, Data Engineering, or related fieldsBanking domain experience is mandatory, especially in areas related to regulatory reporting, compliance, and enterprise data governance"
  },
  {
    "title": "Senior Data Engineer",
    "company": "Mastercard",
    "location": "Pune, Maharashtra, India",
    "description": "Job Title:\n\nSenior Data Engineer\n\nOverview:\n\nPosition Overview:\n\nThe Senior Data Engineer, MyMPA will be part of GBSC\u2019s Automation & Engineering Team, responsible for implementing enhancements and periodic refreshes of an enterprise-wide data platform.\n\nThis role will also work closely with the VP of Analytics & Metrics and Director of FP&A and gather requirements for changes and enhancements to the application and contribute to the technology platform's evolution as it grows to support the rapidly expanding Mastercard business.\n\nThe ideal candidate will have hands-on development skills combined with an ability to analyze and understand end user requirements that are critical success factors within this role. This role requires the skills and desire to work as an individual contributor as well as, collaborate cross functionally with various business constituents.\n\n1. Have you ever worked on an enterprise-wide reporting solution that relied heavily on your own knowledge and abilities to build and maintain the solution?\n2. Are you constantly hungry to learn? Do you have the \u201cgrowth mindset\u201d as opposed to the \u201cfixed mindset\u201d?\n3. Do you love working with people, helping them, and turning their requirements into something that can make a difference?\n\nRole:\n\n\u2022 Skilled at writing performant and reliable SQL queries to source data from Centralized data repositories such as a Data Warehouse.\n\u2022 Proficient at designing and developing Relational and Multi-dimensional Databases to host the sourced data and work with Tableau and Power BI developers within and outside the team to support reporting and dashboarding needs of the organization.\n\u2022 Develop ETL workflows and macros in Alteryx to source data for projects and build high-level and detailed data validations to ensure accuracy of the sourced data\n\u2022 Contribute to initiatives aimed at automating the data extraction, application of data quality checks to ensure datasets are released on time and with 100% accuracy.\n\u2022 Develop technical components to ensure department\u2019s compliance with audit requirements such as SOX and other statutory/audit requirements as applicable from time to time.\n\u2022 Organize and lead discussions with customers to brainstorm on data quality issues and contribute to discussions to devise business rules to address data quality issues\n\u2022 Use MS-Excel and MS-PPT to capture findings and present to customers in an easy-to-understand and impactful manner\n\u2022 Liaison with the internal groups in MasterCard Operations and Technology to ensure our solutions remain in compliance with MasterCard technical standards. Navigate O&T requirements around change management and new development.\n\nAll About You:\n\n\u2022 Strong understanding of Windows and Linux server.\n\u2022 Good understanding of SQL Server or Oracle DB.\n\u2022 Solid understanding of Essbase technology \u2013 understand how this technology works, for both BSO\nand ASO cubes.\n\u2022 Develop BSO and ASO cubes with a strong eye for performance.\n\u2022 Strong commitment to quality and error testing code you develop. Strong ability to step in and analyze the code of others on the team.\n\u2022 Be able to work within an Agile environment that is highly responsive to the business. Our team is part of the Finance organization \u2013 you must be comfortable with working as part of the business with a strong \u201croll up your sleeves\u201d mentality."
  },
  {
    "title": "Engineer III Consultant-Data Engineering",
    "company": "Verizon",
    "location": "Hyderabad, Telangana, India (+2 others)",
    "description": "When you join Verizon\n\nYou want more out of a career. A place to share your ideas freely \u2014 even if they\u2019re daring or different. Where the true you can learn, grow, and thrive. At Verizon, we power and empower how people live, work and play by connecting them to what brings them joy. We do what we love \u2014 driving innovation, creativity, and impact in the world. Our V Team is a community of people who anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together \u2014 lifting our communities and building trust in how we show up, everywhere & always. Want in? Join the #VTeamLife.\n\nWhat you\u2019ll be doing\u2026\n\nWe are looking for data engineers who can work with world class team members to help drive telecom business to its full potential. We are building data products / assets for telecom wireless and wireline business which includes consumer analytics, telecom network performance and service assurance analytics etc. We are working on cutting edge technologies like digital twin to build these analytical platforms and provide data support for varied AI ML implementations.\n\nAs a data engineer you will be collaborating with business product owners, coaches, industry renowned data scientists and system architects to develop strategic data solutions from sources which includes batch, file and data streams\n\nAs a Data Engineer with ETL/ELT expertise for our growing data platform & analytics teams, you will understand and enable the required data sets from different sources both structured and unstructured data into our data warehouse and data lake with real-time streaming and/or batch processing to generate insights and perform analytics for business teams within Verizon.\n\u2022 Understanding the business requirements and converting them to technical design.\n\u2022 Working on Data Ingestion, Preparation and Transformation.\n\u2022 Developing data streaming applications.\n\u2022 Debugging the production failures and identifying the solution.\n\u2022 Working on ETL/ELT development.\n\u2022 Understanding devops process and contributing for devops pipelines\n\nWhat we\u2019re looking for...\n\nYou\u2019re curious about new technologies and the game-changing possibilities it creates. You like to stay up-to-date with the latest trends and apply your technical expertise to solving business problems.\n\nYou\u2019ll need to have\u2026\n\u2022 Bachelor\u2019s degree or four or more years of work experience.\n\u2022 Four or more years of relevant work experience.\n\u2022 Experience with Data Warehouse concepts and Data Management life cycle.\n\u2022 Experience in Big Data technologies - GCP/Hadoop/Spark/Composer/DataFlow/Bigquery.\n\u2022 Experience in complex SQL.\n\u2022 Experience working on Streaming ETL pipelines\n\u2022 Expertise in Java\n\u2022 Experience with MemoryStore / Redis / Spanner\n\u2022 Experience in troubleshooting the data issues.\n\u2022 Experience with data pipeline and workflow management & Governance tools.\n\u2022 Knowledge of Information Systems and their applications to data management processes.\n\nEven better if you have one or more of the following\u2026\n\u2022 Three or more years of relevant experience.\n\u2022 Any relevant Certification on ETL/ELT developer.\n\u2022 Certification in GCP-Data Engineer.\n\u2022 Accuracy and attention to detail.\n\u2022 Good problem solving, analytical, and research capabilities.\n\u2022 Good verbal and written communication.\n\u2022 Experience presenting to and influence stakeholders.\n\u2022 Experience in driving a small team of 2 or more members for technical delivery\n\n#AI&D\n\nWhere you\u2019ll be working\nIn this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.\n\nScheduled Weekly Hours\n40\n\nEqual Employment Opportunity\n\nVerizon is an equal opportunity employer. We evaluate qualified applicants without regard to race, gender, disability or any other legally protected characteristics."
  },
  {
    "title": "Sr. Data and AI Engineer",
    "company": "Philips",
    "location": "Bengaluru, Karnataka, India",
    "description": "Job Title\nSr. Data and AI Engineer\n\nJob Description\n\nYour role:\n\u2022 Analyzes complex datasets to identify key trends, patterns, and potential data quality issues that could impact model performance or downstream analytics, working under general supervision.\n\u2022 Develops and implements efficient data pipelines to extract, transform, and load data from various sources, ensuring data integrity, consistency, and adherence to data governance standards.\n\u2022 Deploys machine learning and AI models in production environments and develops approaches for AI DevOps, adhering to best practices for security, scalability, and model explainability which may involve containerization and orchestration for robust deployments.\n\u2022 Monitors and maintains data pipelines and AI models, proactively identifying and resolving performance bottlenecks or potential issues to ensure continuous functionality and optimal model performance.\n\u2022 Documents data pipelines, models, and processes thoroughly, ensuring clarity, maintainability, and effective knowledge transfer within the team. This documentation should be tailored for both technical and non-technical audiences.\n\u2022 Troubleshoots data quality problems, performs root cause analysis to identify the source of data quality issues and collaborate with data scientists to design and implement effective solutions.\n\u2022 Communicates effectively technical findings and insights to both technical and non-technical audiences, tailoring communication style and detail level to the specific audience.\n\u2022 Learns and adapts skillset by staying up-to-date on the latest advancements in data engineering and AI technologies, explores new tools and techniques to improve ability to deliver efficient and impactful data solutions.\n\u2022 Works with business teams to understand their data needs and challenges and collaborates with data scientists to translate those needs into well-defined technical requirements and actionable data solutions.\n\u2022 Supports data scientists throughout the entire AI lifecycle by preparing data for analysis, building and optimizing data infrastructure for specific needs, and automating data workflows to streamline the model development process.\n\nMinimum required Education:\nBachelor's / Master's Degree in Computer Science, Information Management, Data Science, Econometrics, Artificial Intelligence, Applied Mathematics, Statistics or equivalent.\n\nMinimum required Experience:\nMinimum 2 years of experience with Bachelor's in areas such as Data Handling, Data Analytics, AI Modeling or equivalent OR no prior experience required with Master's Degree.\n\nPreferred Experience:\n10 years + Experience in using programming languages such as Python, R, JAVA, C/C++\n\nPreferred Certification:\nArtificial Intelligence Board of America (ARTiBA) certified\n\nHow we work together\n\nWe believe that we are better together than apart. For our office-based teams, this means working in-person at least 3 days per week.\nthis role is an office role.\n\nAbout Philips\nWe are a health technology company. We built our entire company around the belief that every human matters, and we won't stop until everybody everywhere has access to the quality healthcare that we all deserve. Do the work of your life to help the lives of others.\n\u2022 Learn more about our business.\n\u2022 Discover our rich and exciting history.\n\u2022 Learn more about our purpose.\nIf you\u2019re interested in this role and have many, but not all, of the experiences needed, we encourage you to apply. You may still be the right candidate for this or other opportunities at Philips. Learn more about our culture of impact with care here.\n\n#LI-EU\n\n#LI-Hybrid\n\n#LI-PHILIN"
  },
  {
    "title": "Data Engineer  IRC265240",
    "company": "Hitachi Careers",
    "location": "Hyderabad, Telangana, India",
    "description": "Description\n\nGL\n\nRequirements\n\nTotal of 4-6 years of development/design experience with a minimum of 3 years experience in Big Data technologies on-prem and on cloud.\nProficiency in Snowflake and strong SQL programming skills.\nStrong experience with data modeling and schema design.\nExtensive experience in using Data warehousing tool like Snowflake/BigQuery/RedShift.\nExtensive experience with BI Tools like Tableau/QuickSight/PowerBI. At least one must have.\nMust have experience with orchestration tools like Airflow and transformation tool DBT.\nStrong Experience implementing ETL/ELT processes and building data pipelines including workflow management, job scheduling and monitoring\nGood understanding of Data Governance, Security and Compliance, Data Quality, Metadata Management, Master Data Management, Data Catalog\nStrong understanding of cloud services (AWS), including IAM and log analytics.\nExcellent interpersonal and teamwork skills\nExperience with leading and mentorship of other team members\nGood knowledge of Agile Scrum\nGood communication skills\n\nJob responsibilities\n\nSame as above\n\nWhat we offer\n\nCulture of caring. At GlobalLogic, we prioritize a culture of caring. Across every region and department, at every level, we consistently put people first. From day one, you'll experience an inclusive culture of acceptance and belonging, where you'll have the chance to build meaningful connections with collaborative teammates, supportive managers, and compassionate leaders.\n\nLearning and development. We are committed to your continuous learning and development. You'll learn and grow daily in an environment with many opportunities to try new things, sharpen your skills, and advance your career at GlobalLogic. With our Career Navigator tool as just one example, GlobalLogic offers a rich array of programs, training curricula, and hands-on opportunities to grow personally and professionally.\n\nInteresting & meaningful work. GlobalLogic is known for engineering impact for and with clients around the world. As part of our team, you'll have the chance to work on projects that matter. Each is a unique opportunity to engage your curiosity and creative problem-solving skills as you help clients reimagine what's possible and bring new solutions to market. In the process, you'll have the privilege of working on some of the most cutting-edge and impactful solutions shaping the world today.\n\nBalance and flexibility. We believe in the importance of balance and flexibility. With many functional career areas, roles, and work arrangements, you can explore ways of achieving the perfect balance between your work and life. Your life extends beyond the office, and we always do our best to help you integrate and balance the best of work and life, having fun along the way!\n\nHigh-trust organization. We are a high-trust organization where integrity is key. By joining GlobalLogic, you're placing your trust in a safe, reliable, and ethical global company. Integrity and trust are a cornerstone of our value proposition to our employees and clients. You will find truthfulness, candor, and integrity in everything we do.\n\nAbout GlobalLogic\n\nGlobalLogic, a Hitachi Group Company, is a trusted digital engineering partner to the world's largest and most forward-thinking companies. Since 2000, we've been at the forefront of the digital revolution - helping create some of the most innovative and widely used digital products and experiences. Today we continue to collaborate with clients in transforming businesses and redefining industries through intelligent products, platforms, and services."
  },
  {
    "title": "Data Engineer II Development",
    "company": "Thermo Fisher Scientific",
    "location": "Bengaluru, Karnataka, India",
    "description": "Work Schedule\nStandard (Mon-Fri)\n\nEnvironmental Conditions\nOffice\n\nJob Description\n\nJob Title \u2013Data Engineer II, Development\n\nJob Location: Bangalore, India\nAbout Company:\n\nThermo Fisher Scientific Inc. is the world leader in serving science, with revenues of more than $40 billion and approximately 1,20,000 employees globally. Our Mission is to enable our customers to make the world healthier, cleaner, and safer. We help our customers accelerate life sciences research, solve complex analytical challenges, improve patient diagnostics, deliver medicines to market and increase laboratory productivity.\n\nAbout Team: We are Automation, AI and Data (AAD) team that caters to data engineering and analytics, automation and AI solutions for various groups and divisions within Thermo fisher Scientific.\n\nWhat will you do?\n\nAs a data engineer, you will play a key role in strengthening our data engineering capabilities delivering data engineering pipelines and solutions through Enterprise Data Platform (EDP) for various groups and divisions.\nGeneral Job Functions\n\u2022 Perform source to target data analysis and mappings.\n\u2022 Build, test, and optimize data pipelines for various use-cases, including real-time and batch processing, based on specific requirements.\n\u2022 Collaborate with admins, developers, data engineers and analysts for successful functionality delivery\n\u2022 Perform requirement analysis and co-ordinate with project managers and development team to drive the delivery cycle.\n\u2022 Collaborate with the scrum master on product backlogs and helps with sprint planning.\n\u2022 Supports the evolution of EDP architecture and takes part in roadmap activities around data platform architecture initiatives or changes.\n\u2022 Collaborate with leadership and stakeholders to ensure data quality and integrity in DWH & AWS platforms for BI/Analytical reporting.\n\u2022 Pre-meditate any risks well in advance and communicate to stakeholders to formulate the plan of action to mitigate any risks.\n\u2022 Follow agile development methodologies to deliver solutions and product features by following DevOps practices.\n\u2022 Ensure the teams follow the prescribed development processes and approaches.\nMust have skills and experience\n\u2022 4+ years of exclusive experience in delivering data solutions.\n\u2022 3+ years of proficient experience in building data pipelines in Databricks.\n\u2022 3+ years of proven experience building Cloud BI solutions using AWS or Azure.\n\u2022 Experience with agile development methodologies by following DevOps, Data Ops and Dev Sec Ops practices.\n\u2022 3+ years of programming in SQL, Pyspark and Python.\n\u2022 Excellent written, verbal and interpersonal and partner communication skills.\n\u2022 Excellent Analysis and business requirement documentation skills.\n\u2022 Ability to work with multi-functional teams from multiple regions/ time zones by optimally demonstrating multi-form communication (Email, MS Teams for voice and chat, meetings)\n\u2022 Excellent prioritization and problem-solving skills.\nGood to have skills\n\u2022 Relevant certifications in data engineering on cloud platforms.\n\u2022 Good understanding of machine learning and Generative AI concepts.\n\u2022 Hands on experience with Snowflake or Azure data engineering.\n\u2022 Knowledge of SQL and NoSQL databases like PostgreSQL, MySQL, MongoDB Cassandra.\n\u2022 Data visualization experience using tools such as Power BI or Tableau.\n\u2022 Knowledge of data governance practices, data quality, and data security.\n\nOur Mission is to enable our customers to make the world healthier, cleaner, and safer. Watch as our colleagues explain 5 reasons to work with us. As one team of 1,20,000 colleagues, we share a common set of values - Integrity, Intensity, Innovation and Involvement - working together to accelerate research, solve complex scientific challenges, drive technological innovation and support patients in need.\n\nThermoFisher Scientific is an EEO/Affirmative Action Employer and does not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability or any other legally protected status."
  },
  {
    "title": "Data Engineer",
    "company": "Advance Auto Parts",
    "location": "Telangana, India",
    "description": "Job Description\n\nWHO WE ARE\u202f\n\nCome join our Technology Team and start reimagining the future of the automotive aftermarket. We are a highly motivated tech-focused organization, excited to be amid dynamic innovation and transformational change. Driven by Advance\u2019s top-down commitment to empowering our team members, we are focused on delighting our Customers with Care and Speed, through delivery of world class technology solutions and products.\u202f\u202f\n\nWe value and cultivate our culture by seeking to always be collaborative, intellectually curious, fun, open, and diverse.\u202f\u202f You will be a key member of a growing and passionate group focused on collaborating across business and technology resources to drive forward key programs and projects building enterprise capabilities across Advance Auto Parts.\u202f\u202f\n\nTHE OPPORTUNITY:\n\nJoin the AAP team and start reimagining the future of automotive retail. Disrupt the way consumers buy auto parts and\u202ftake on the industry\u2019s biggest challengers\u202fto\u202fexecute on AAP's top-down\u202fcommitment to digital expansion.\u202f\n\nAs a member of the Advance Auto Parts team, you will have an opportunity to disrupt a $150B auto parts industry to bring better and faster solutions to customers. You will be part of a team helping the company live its mission of \u201cAdvancing a World in Motion\u201d. The role is part of a\u202fmerit-based organization with\u202fa culture of professional\u202fgrowth and\u202fdevelopment,\u202fand emphasis on the latest tools, platforms and technologies.\u202f\n\nResponsibilities:\n\u2022 Lead the migration and modernization of data platforms, moving applications and pipelines to Google Cloud-based solutions.\n\u2022 Architect and maintain cloud-based data infrastructure leveraging AWS or GCP services.\n\u2022 Ensure data security and governance, enforcing compliance with industry standards and regulations.\n\u2022 Develop and promote best practices for data modeling, processing, and analytics. Mentor and guide a team of data engineers, fostering a culture of innovation and technical excellence.\n\u2022 Manage and scale data pipelines from internal and external data sources to support new product launches and ensure high data quality.\n\u2022 Develop automation and monitoring frameworks to capture key metrics and operational KPIs for pipeline performance.\n\u2022 Collaborate with internal teams, including data science and product teams, to drive solutioning and proof-of-concept (PoC) discussions.\n\u2022 Develop and optimize procedures to transition data into production.\n\u2022 Define and manage SLAs for data products and operational processes.\n\u2022 Research and apply state-of-the-art methodologies in data and Platform engineering.\n\u2022 Create and maintain technical documentation for sharing knowledge.\n\u2022 Develop reusable packages and libraries to enhance development efficiency.\n\u2022 Lead and drive the development and optimization of scalable data architectures and pipelines.\n\u2022 Develop real-time and batch data processing solutions, integrating structured and unstructured data sources.\n\nRequired Qualification:\n\u2022 We are looking for a candidate with 5-8 years of experience in Data Engineering and Application development. They must have a graduate degree in Computer Science or a related field of study. They must have experience with programming languages such as Python, Java & DS&Algo, Spark, and Scala. Expertise in Python and Spark is a must.\n\u2022 2 + years of AWS and Cloud technologies. Experience in data platform engineering, with a focus on cloud transformation and modernization.\n\u2022 Hands-on experience building large, scaled data pipelines in cloud environments and handling of data in PBs.\n\u2022 Experience with CI/CD pipeline management in GCP DevOps.\n\u2022 Understanding of data governance, security, and compliance best practices.\n\u2022 Experience working in an Agile development environment.\n\u2022 Prior experience in migrating applications from legacy platforms to the cloud.\n\u2022 Knowledge of Terraform or Infrastructure-as-Code (IaC) for cloud resource management.\n\u2022 Familiarity with Kafka, Event Hubs, or other real-time data streaming solutions.\n\u2022 Experience with legacy RDBMS (Oracle, DB2, Teradata) & DataStage/Talend\n\u2022 Having background supporting data science models in production.\n\nCalifornia Residents click below for Privacy Notice:\nhttps://jobs.advanceautoparts.com/us/en/disclosures"
  },
  {
    "title": "Senior Analytics Data Engineer",
    "company": "Okta, Inc.",
    "location": "Bengaluru, Karnataka, India",
    "description": "Get to know OktaOkta is The World\u2019s Identity Company. We free everyone to safely use any technology, anywhere, on any device or app. Our flexible and neutral products, Okta Platform and Auth0 Platform, provide secure access, authentication, and automation, placing identity at the core of business security and growth.At Okta, we celebrate a variety of perspectives and experiences. We are not looking for someone who checks every single box - we\u2019re looking for lifelong learners and people who can make us better with their unique experiences. Join our team! We\u2019re building a world where Identity belongs to you.Senior Analytics Engineer\nWe are looking for an experienced Analytics Engineer to join Okta\u2019s enterprise data team. This analyst will have strong background in SaaS subscription and product analytics, a passion for providing customer usage insights to internal stakeholders, and experience organizing complex data into consumable data assets. In this."
  },
  {
    "title": "Senior Azure Data Engineer",
    "company": "DXC Technology",
    "location": "Bengaluru, Karnataka, India",
    "description": "Job Description:\n\nSenior Azure Data Engineer\n\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\u2022 At least 5+ years\u2019 of relevant hands on development experience as Azure Data Engineering role\n\u2022 Proficient in Azure technologies like ADB, ADF, SQL(capability of writing complex SQL queries), ADB, PySpark, Python, Synapse, Delta Tables, Unity Catalog\n\u2022 Hands on in Python, PySpark or Spark SQL\n\u2022 Hands on in Azure Analytics and DevOps\n\u2022 Taking part in Proof of Concepts (POCs) and pilot solutions preparation\n\u2022 Ability to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows\n\u2022 Experience in business processing mapping of data and analytics solutions\n\nAt DXC Technology, we believe strong connections and community are key to our success. Our work model prioritizes in-person collaboration while offering flexibility to support wellbeing, productivity, individual work styles, and life circumstances. We\u2019re committed to fostering an inclusive environment where everyone can thrive.\n\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here."
  }
]